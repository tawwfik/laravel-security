<?php

declare (strict_types = 1);

namespace Tawfik\LaravelSecurity\Commands;

/**
 * Secure Robots.txt Command
 *
 * Generates a robots.txt file to:
 * - Prevent crawling of sensitive directories
 * - Add crawl-delay for aggressive bots
 * - Include sitemap reference
 * - Block access to configuration files
 */
class SecureRobots extends BaseSecurityCommand
{
    protected $signature = 'secure:robots
                            {--dry-run : Show what would be changed without making changes}
                            {--backup : Create backup before modifications}
                            {--sitemap= : Sitemap URL}
                            {--crawl-delay= : Crawl delay in seconds}';

    protected $description = 'Generate a secure robots.txt file to prevent crawling of sensitive directories';

    protected function executeCommand(): void
    {
        $this->info('ðŸ”’ Generating secure robots.txt file...');

        if (! $this->validateLaravelApp()) {
            return;
        }

        $robotsPath = $this->getPublicPath() . '/robots.txt';

        // Backup existing robots.txt if it exists
        if ($this->fileExists($robotsPath) && $this->option('backup')) {
            $this->backupFile($robotsPath);
        }

        // Generate robots.txt content
        $content = $this->generateRobotsTxtContent();

        if ($this->writeFile($robotsPath, $content)) {
            $this->addResult('Created', 'Secure robots.txt file');
            $this->addResult('Protection', 'Blocked crawling of sensitive directories');
            $this->addResult('Features', 'Crawl-delay, sitemap reference, security-focused');
        }
    }

    /**
     * Generate robots.txt content
     */
    private function generateRobotsTxtContent(): string
    {
        $content = "# Laravel Security - Robots.txt\n";
        $content .= "# Generated by Laravel Security Package\n";
        $content .= "# " . date('Y-m-d H:i:s') . "\n\n";

        // User-agent for all bots
        $content .= "User-agent: *\n";

        // Disallow sensitive directories and files
        $disallowPaths = config('laravel-security.robots.disallow', []);
        foreach ($disallowPaths as $path) {
            $content .= "Disallow: {$path}\n";
        }

        // Add crawl delay
        $crawlDelay = $this->option('crawl-delay') ?? config('laravel-security.robots.crawl_delay', 10);
        $content .= "Crawl-delay: {$crawlDelay}\n\n";

        // Sitemap reference
        $sitemap = $this->option('sitemap') ?? config('laravel-security.robots.sitemap', 'https://example.com/sitemap.xml');
        $content .= "Sitemap: {$sitemap}\n\n";

        // Additional security notes
        $content .= "# Security Notes:\n";
        $content .= "# - This robots.txt prevents crawling of sensitive application files\n";
        $content .= "# - Configuration files, logs, and backups are protected\n";
        $content .= "# - Crawl-delay helps prevent server overload\n";
        $content .= "# - Sitemap helps legitimate crawlers find public content\n\n";

        // Block specific malicious bots
        $content .= "# Block malicious bots\n";
        $content .= "User-agent: sqlmap\n";
        $content .= "Disallow: /\n\n";

        $content .= "User-agent: nikto\n";
        $content .= "Disallow: /\n\n";

        $content .= "User-agent: dirbuster\n";
        $content .= "Disallow: /\n\n";

        $content .= "User-agent: gobuster\n";
        $content .= "Disallow: /\n\n";

        $content .= "User-agent: nmap\n";
        $content .= "Disallow: /\n\n";

        // Allow specific legitimate bots with restrictions
        $content .= "# Allow legitimate bots with restrictions\n";
        $content .= "User-agent: Googlebot\n";
        $content .= "Disallow: /admin/\n";
        $content .= "Disallow: /api/\n";
        $content .= "Disallow: /storage/\n";
        $content .= "Crawl-delay: 1\n\n";

        $content .= "User-agent: Bingbot\n";
        $content .= "Disallow: /admin/\n";
        $content .= "Disallow: /api/\n";
        $content .= "Disallow: /storage/\n";
        $content .= "Crawl-delay: 1\n\n";

        return $content;
    }
}
